[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ssi_pingme",
    "section": "",
    "text": "#Create the dev environment in ./.venv then activate it so you're working in it\nconda env create -p ./.venv --file conda.dev.env.yaml\nconda activate ./.venv\n#Need to add kernel to jupyterhub for processing on server, ipykernel is part of conda.dev.env.yaml\npython -m ipykernel install --user --name=Pingme\n#After this change your kernel to Pingme via JupyterHub (top right of a notebook)\n\n#Now we need to create the repo locally\nnbdev_export\n\n#with it created we can install the dev env\npip install -e '.[dev]'\n#optional test to ensure it worked\ncore_hello_world test #to test everything works\n\n#now we can do nbdev_prepare\nnbdev_prepare\nIf releasing to conda be sure to add the channel povilasmat to your conda for envyaml\n\n\npip install ssi_pingme\n\n\n\nTo run as a webservice, be sure the config_file has the webhook param!\n\n!pingme_start_webservice -h\n\nzsh:1: command not found: start_pingme_webservice\n\n\n\n!pingme_webhook_default -h\n\nusage: pingme_webhook_default [-h] [--config_file CONFIG_FILE]\n\noptions:\n  -h, --help                 show this help message and exit\n  --config_file CONFIG_FILE\n\n\n\n!pingme_webhook_simple -h\n\nusage: pingme_webhook_simple [-h] [--config_file CONFIG_FILE] title text\n\npositional arguments:\n  title\n  text\n\noptions:\n  -h, --help                 show this help message and exit\n  --config_file CONFIG_FILE\n\n\n\n!pingme_webhook_card -h\n\nusage: pingme_webhook_card [-h] [--config_file CONFIG_FILE] card\n\npositional arguments:\n  card\n\noptions:\n  -h, --help                 show this help message and exit\n  --config_file CONFIG_FILE",
    "crumbs": [
      "ssi_pingme"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "ssi_pingme",
    "section": "",
    "text": "pip install ssi_pingme",
    "crumbs": [
      "ssi_pingme"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "ssi_pingme",
    "section": "",
    "text": "To run as a webservice, be sure the config_file has the webhook param!\n\n!pingme_start_webservice -h\n\nzsh:1: command not found: start_pingme_webservice\n\n\n\n!pingme_webhook_default -h\n\nusage: pingme_webhook_default [-h] [--config_file CONFIG_FILE]\n\noptions:\n  -h, --help                 show this help message and exit\n  --config_file CONFIG_FILE\n\n\n\n!pingme_webhook_simple -h\n\nusage: pingme_webhook_simple [-h] [--config_file CONFIG_FILE] title text\n\npositional arguments:\n  title\n  text\n\noptions:\n  -h, --help                 show this help message and exit\n  --config_file CONFIG_FILE\n\n\n\n!pingme_webhook_card -h\n\nusage: pingme_webhook_card [-h] [--config_file CONFIG_FILE] card\n\npositional arguments:\n  card\n\noptions:\n  -h, --help                 show this help message and exit\n  --config_file CONFIG_FILE",
    "crumbs": [
      "ssi_pingme"
    ]
  },
  {
    "objectID": "pingme_api.html",
    "href": "pingme_api.html",
    "title": "pingme API",
    "section": "",
    "text": "Included Libraries\nLaunch the app to run things, using default params which includes /docs for swagger ui\n\nsource\n\nwebhook_card_default\n\n webhook_card_default ()\n\nSend a default card to the webhook, intention is strictly for testing and showcasing.\n\nclient = TestClient(app)\nresponse = client.post(\"/webhook/default\")\nprint(response, json.dumps(response.json(), indent=2))\n\n\nsource\n\n\nwebhook_card_simple\n\n webhook_card_simple (title:str, text:str)\n\n*Send a simple card to the webhook, should be used for most general use cases of sending a message.\nArgs: title: Title of the card text: Text of the card*\n\nclient = TestClient(app)\nresponse = client.post(\"/webhook/simple?title=TestingTitle&text=TestText\")\nprint(response, json.dumps(response.json(), indent=2))\n\n\nsource\n\n\nwebhook_card\n\n webhook_card (card:pingme.pingme_class.Card)\n\n*Send a card to the webhook, card defines a card thats installed into the config.yaml. Advanced usage which may not get used.\nArgs: card: Card object*\n\n# Test wont work for runner with default config\nclient = TestClient(app)\nresponse = client.post(\n    \"/webhook/card\",\n    json={\n        \"name\": \"default\",\n        \"context\": {\"title\": \"titlevalue\", \"text\": \"textvalue\"},\n    },\n)\nprint(response, json.dumps(response.json(), indent=2))\n\n\nsource\n\n\nhelp\n\n help ()\n\nInformation on how to use API to users, when they hit the root URL. Ensure when you add new endpoints to update this function, check /docs for info on endpoints.\n\n# Test\nclient = TestClient(app)\nresponse = client.get(\"/\")\nprint(response.status_code, json.dumps(response.json(), indent=2))\n\n\nsource\n\n\nwebservice\n\n webservice (host:str='127.0.0.1', port:int=5000, config_file:str=None)\n\nStart the PingMe API server.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhost\nstr\n127.0.0.1\nHost to run the server on\n\n\nport\nint\n5000\nPort to run the server on”\n\n\nconfig_file\nstr\nNone\nPath to config file”",
    "crumbs": [
      "pingme API"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Global static vars",
    "section": "",
    "text": "For help with the Markdown language, see this guide.\nsource",
    "crumbs": [
      "Global static vars"
    ]
  },
  {
    "objectID": "core.html#libraries",
    "href": "core.html#libraries",
    "title": "Global static vars",
    "section": "Libraries",
    "text": "Libraries\nCurrently all libraries included are listed at the top and calls to them are also made in the block of code that uses them. This is for readability and the performance hit of the import is negligible.",
    "crumbs": [
      "Global static vars"
    ]
  },
  {
    "objectID": "core.html#config",
    "href": "core.html#config",
    "title": "Global static vars",
    "section": "Config",
    "text": "Config\nOur config file holds all program and user specific variables. This is a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production. This is also a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production.\nConfiguration is templated to rely on environment (ENV) variables. A default ENV config is provided in ./config/config.default.env and more advanced data structures are supported in ./config/config.default.yaml. The .yaml file is meant to represent what your program actually works with and the .env file options the user can change at run time.\nMake sure you know the priority of variables and check on them when debugging your code. Also ensure that your yaml file is referenced appropriately in the .env file.\nWhen in use there’s an expectation you’ll have multiple config files for different use cases e.g. development, production environment for different paths, etc.\n\nset env variables\nA helper function for getting your config values, this will set the environment variables with the provided .env values. If you’re missing values it’ll ensure they’re loaded in with the defaults file.\n\nsource\n\n\nset_env_variables\n\n set_env_variables (config_path:str, overide_env_vars:bool=True)\n\n*Load dot env sets environmental values from a file, if the value already exists it will not be overwritten unless override is set to True. If we have multiple .env files then we need to apply the one which we want to take precedence last with overide.\nOrder of precedence: .env file &gt; environment variables &gt; default values When developing, making a change to the config will not be reflected until the environment is restarted\nSet the env vars first, this is needed for the card.yaml to replace ENV variables NOTE: You need to adjust PROJECT_NAME to your package name for this to work, the exception is only for dev purposes This here checks if your package is installed, such as through pypi or through pip install -e [.dev] for development. If it is then it’ll go there and use the config files there as your default values.\nArgs: config_path (str): path to the config file overide_env_vars (bool): if True, will overwrite existing env variables\nReturns: bool: True if successful, False otherwise*\n\n\nget config\nWhen you run this function, assuming things are set up properly, you end up with a dict that matches your .yaml file. This file will have all the inputs for the package and settings of your program.\nTo do this it will use a .env config file, which has an associated yaml file defined with CORE_YAML_CONFIG_FILE in the .env file. And then use the .env file to load values into the associated .yaml file.\n\nsource\n\n\nget_config\n\n get_config (config_path:str=None, overide_env_vars:bool=True)\n\n*Load the config.env from the config path, the config.env should reference the config.yaml file, which will be loaded and returned as a dictionary. The config.yaml file should be in the same directory as the config.env file.\nArgs: config_path (str): The path to the config.env file overide_env_vars (bool): If the env vars should be overriden by the config.yaml file\nReturns: dict: The config.yaml file as a dictionary, it’ll also replace any ENV variables in the yaml file*\n\n\nVariables\nAll the user input variables and machine adjustable variables should be in your config, which is a dict. Reference config.default.yaml for how to access your variables. Also note that with python dicts you can use dict_variable.get(\"variable\", default_value) to ensure that you don’t get a key error if the variable is not set.\n\n\nshow project env vars\nA helper function intended to only be used with debugging. It shows all your project specific environmental variables.\n\nsource\n\n\nshow_project_env_vars\n\n show_project_env_vars (config:dict)\n\n*Show all the project environment variables, this is useful for debugging and seeing what is being set\nArgs: config (dict): The dictionary of all the environment variables\nReturns: None*\n\nsource\n\n\ntool_is_present\n\n tool_is_present (tool_name:str)\n\n*Check if a tool is present in the current environment\nArgs: tool_name (str): The name of the tool to check\nReturns: bool: True if the tool is present, False otherwise*\n\nsource\n\n\ntools_are_present\n\n tools_are_present (tool_names:list)\n\n*Check if all tools are present in the current environment\nArgs: tool_names (list): A list of tools to check\nReturns: bool: True if all tools are present, False otherwise*",
    "crumbs": [
      "Global static vars"
    ]
  },
  {
    "objectID": "core.html#get_samplesheet",
    "href": "core.html#get_samplesheet",
    "title": "Global static vars",
    "section": "get_samplesheet",
    "text": "get_samplesheet\nThis function is to unify the way we work with sample_sheet’s which is for us a file with a table of values, typically samples for batch processing. We want to approach doing it this way so all programs have batch processing in mind and working with the same data structure.\nTo make use of it we have a small sample_sheet yaml object which looks like\nsample_sheet:\n    path: path/to/sample_sheet.tsv\n    delimiter: '\\t' # Optional, will assume , for csv and \\t otherwises\n    header: 0 # Optional, 0 indicates first row is header, None indicates no header\n    columns: ['column1', 'column2', 'column3'] # Optional, if not provided all columns will be used\nMake sure to add that to your relevant section in your config (can be multiple times if you’re working with different sheets or different columns), then call the function on this object and it’ll either mention somethings wrong or return a pandas dataframe with the columns of interest.\nThis is an example of a common sample_sheet we work with. We will ingest the hash at the beginning so it doesn’t affect column naming. Extra empty rows at the end are also stripped.\n#sample_id  file_path   metadata1   metadata2\nSample1 /path/to/sample1.fasta  value1  option1\nSample2 /path/to/sample2.fasta  value2  option2\nSample3 /path/to/sample3.fasta  value3  option1\nSample4 /path/to/sample4.fasta  value1  option2\nSample5 /path/to/sample5.fasta  value2  option1\n\nsource\n\nget_samplesheet\n\n get_samplesheet (samplesheet_config:__main__.SamplesheetConfig)\n\n*Load the sample sheet into a pandas dataframe If columns is not None then it will only load those columns If the sample sheet is a csv then it will load it as a csv, otherwise it will assume it’s a tsv\nExpected samplesheet_config: sample_sheet: path: path/to/sample_sheet.tsv delimiter: ’ ’ # Optional, will assume , for csv and otherwises header: 0 # Optional, 0 indicates first row is header, None indicates no header columns: [‘column1’, ‘column2’, ‘column3’] # Optional, if not provided all columns will be used\nExample sample sheet: #sample_id file_path metadata1 metadata2 Sample1 /path/to/sample1.fasta value1 option1 Sample2 /path/to/sample2.fasta value2 option2 Sample3 /path/to/sample3.fasta value3 option1 Sample4 /path/to/sample4.fasta value1 option2 Sample5 /path/to/sample5.fasta value2 option1\nArgs: samplesheet_config (SamplesheetConfig): The configuration for loading the sample sheet\nReturns: pd.DataFrame: The sample sheet as a pandas dataframe*\n\nsource\n\n\nSamplesheetConfig\n\n SamplesheetConfig (config:dict[str,typing.Any])\n\n*Configuration class for loading a sample sheet into a pandas dataframe\nExtends: BaseModel*\n\nconfig_dict = {\n    \"path\": f\"{PROJECT_DIR}/input/example_samplesheet.tsv\",\n    \"delimiter\": \"\\t\",\n    \"columns\": None,\n}\n\nsamplesheet_config = SamplesheetConfig(config_dict)\n\nprint(get_samplesheet(samplesheet_config))\n\nThe functions below are not tempalted and you should adjust this with your own code. It’s included as an example of how to code some functions with associated tests and how to make it work on the command line. It is best to code by creating a new workbook and then importing the functions of this into that one.\n\nsource\n\n\nhello_world\n\n hello_world (name:str='Not given')\n\nA simple function that returns a hello world message with a name, for testing purposes\nThis here is a a test as part of fastcore.test, all fastcore tests will be automatically run when doing nbdev_test as well as through github actions.\n\ntest.test_eq(\"Hello World! My name is Kim\", hello_world(\"Kim\"))\n\nThe @call_parse will, with the settings.ini entry way, automatically transform your function into a command line tool. Comments of the functions will appear for help messages and the initial docstring will appear in the help as well. You can also define defaults for the arguments and should define a typehint to control inputs. The function will likely have to resolve variables with ENV vars and config files. The recommended way to do this is to assume variables passed here are a higher priority.\n\nsource\n\n\ncli\n\n cli (name:str, config_file:str=None)\n\n*This will print Hello World! with your name\nArgs: name (str): Your name config_file (str): The path to the config file, if not provided it will use the default config file*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nYour name\n\n\nconfig_file\nstr\nNone\nconfig file to set env vars from\n\n\n\nTest the function with potentially variable input to confirm output\n\ntest.test_eq(\n    \"Hello World! My name is Kim\", hello_world(config[\"example\"][\"input\"][\"name\"])\n)\ntest.test_eq(None, cli(\"Kim\"))\n\n\nsource\n\n\nexecute_job\n\n execute_job (script_path, use_slurm=False, slurm_params=None)\n\nExecutes a bash script either locally (bash) or via Slurm.\n\nsource\n\n\ncreate_bash_script\n\n create_bash_script (script_path, script_content, slurm_params=None)\n\nCreate a bash script with the given content.\n\ndef submit_job(script_path, dependency=None) -&gt; str | None:\n    \"\"\"\n    Submit a Slurm job with optional dependencies.\n\n    Args:\n    script_path (str): Path to the script to submit.\n    dependency (str): Job ID to depend on.\n\n    Returns:\n    str: Job ID if successful, None otherwise.\n    \"\"\"\n    command = [\"sbatch\"]\n    if dependency:\n        command.append(f\"--dependency=afterok:{dependency}\")\n    command.append(script_path)\n\n    result = subprocess.run(command, capture_output=True, text=True)\n    if result.returncode == 0:\n        job_id = result.stdout.strip().split()[-1]  # Extract job ID from output\n        print(f\"Job submitted: {job_id}\")\n        return job_id\n    else:\n        print(f\"Error submitting job: {result.stderr.strip()}\")\n        return None\n\n\ndef submit_workflow(scripts) -&gt; str | None:\n    \"\"\"\n    Submit a series of scripts with dependencies.\n\n    Args:\n    scripts: List of script paths to submit, they will be submitted in order with previous job as a dependency.\n\n    Returns:\n    str: Job ID of the last submitted job if successful, None otherwise.\n    \"\"\"\n    previous_job_id = None\n    for script in scripts:\n        dependency = (  # Construct the dependency argument if there is a previous job\n            f\"--dependency=afterok:{previous_job_id}\" if previous_job_id else None\n        )\n        previous_job_id = submit_job(script, dependency)\n    return previous_job_id",
    "crumbs": [
      "Global static vars"
    ]
  },
  {
    "objectID": "02_pingme_services.html",
    "href": "02_pingme_services.html",
    "title": "ssi_pingme",
    "section": "",
    "text": "Service layer, calls existing functionality from the class and makes an individual function for each interface that will be exposed through the API.\n\nsource\n\nNotificationService\n\n NotificationService ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncard = Card.model_validate(\n    {\n        \"name\": \"default\",\n        \"context\": {\"title\": \"Test Title\", \"text\": \"Test Text\"},\n    }\n)\nnotification = PingMe(\n    card,\n    config_file=settings.config_file,\n)\nresponse = notification.send_webhook()\n\n\nsource\n\n\npingme_send_default_card_to_webhook\n\n pingme_send_default_card_to_webhook (config_file:str=None)\n\n*Send a card to the webhook with default values\nArgs: - config_file: str = None: Path to the config file, none uses default*\n\nsource\n\n\npingme_send_simple_card_to_webhook\n\n pingme_send_simple_card_to_webhook (title:str, text:str,\n                                     config_file:str=None)\n\n*Send a card to the webhook with title and text\nArgs: - title: str: Title of the card - text: str: Text of the card - config_file: str = None: Path to the config file, none uses default*\n\nsource\n\n\npingme_send_card_to_webhook\n\n pingme_send_card_to_webhook (card:pingme.pingme_class.Card,\n                              config_file:str=None)\n\n*Send a card to the webhook\nArgs: - card: Card: Card object to send - config_file: str = None: Path to the config file, none uses default*",
    "crumbs": [
      "02_pingme_services.html"
    ]
  },
  {
    "objectID": "pingme_class.html",
    "href": "pingme_class.html",
    "title": "Additional Learning Resources",
    "section": "",
    "text": "Now that you’ve finished the getting started in ./GETTING_STARTED_WITH_TEMPLATE.md, you’ll notice the directory has many new files and folders. In this example $YOUR_REPO_NAME is template_nbdev_example so be sure to adjust accordingly.\nThere’s now\n./_docs\n.quarto\n./template_nbdev_example # This is the code autogenerated from the notebooks, you should only adjust this through your notebooks\n./template_nbdev_example/__pycache__\n./template_nbdev_example/__init__.py\n./template_nbdev_example/_modidx.py\n./template_nbdev_example/core.py\n./template_nbdev_example/hello_world.py\n./template_nbdev_example.egg-info # This is metadata about the package for pip\n_quarto.yml\nindex.ipynb # This is a notebook for showing how the program works and generated the README.md, you should adjust this\nMANIFEST.in # This determines what files outside of .py files are included in the package, you may need to adjust it.\nREADME.md\nsetup.py # This is the file that tells pip how to install the package, you shouldn't need to edit this ever\nstyles.css\n\nIf you want to check what you’re documentation looks like run nbdev_preview in command line of the project folder\n\n\nLibraries\nHere we include all the libraries of this module. You can see they’re sectioned so the top parts can be easy cut and paste into new files.\nNormally your imports go into Project specific libraries above, but we’ll put it in a code block here. In this example you’ll want to comment out the code below, because YOUR_REPO_NAME changes with each repository, it’ll cause issues if you try to run it with a different repository name\nBecause the notebooks now are located in the nbs folder, we need to change the python wd for the notebook to the project folder. Keep this included in all notebooks but don’t export it to the package.\n\n# This block should never be exported. It is to have python running in the project (and not the nbs) dir, and to initiate the package using pip.\nos.chdir(core.PROJECT_DIR)\n\nNow you have access to your functions in core.py and call call them here.\nNOTE: if you change another notebook, run nbdev_prepare, and restart your current kernel to see the changes\nHere we’ll load the config file values, note that the file isn’t exported so is for development and documentation purposes.\n\nA note on config files: The final package should only contain config.default.* files. These files are located in package dir PACKAGE_DIR/config. For development, custom config files can be specified in the project dir PROJECT_DIR/config/ folder. These files will not be shipped with the package, but for development, they can be accessed from the project dir (thanks to to code above), which usually is your working directory in terminal while developping.\n\nLets look at our values, as we have a dictionary, that can be viewed more nicely as a json object\n\nsource\n\nCard\n\n Card (name:str, context:dict)\n\n*Usage docs: https://docs.pydantic.dev/2.10/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nresolved_payload\n\n resolved_payload (template:&lt;module'json'from'/opt/hostedtoolcache/Python/\n                   3.10.16/x64/lib/python3.10/json/__init__.py'&gt;,\n                   context:dict)\n\n*Resolves the payload by substituting variables in the payload with values from the context and ensures all variables are accounted for\nArgs: template: json, the payload to be resolved context: dict, the values to substitute into the payload\nReturns: json, the resolved payload*\n\nsource\n\n\nPingMe\n\n PingMe (card:__main__.Card, config_file=None)\n\nPingMe class which notifies via either a webhook or email\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncard\nCard\n\n\n\n\nconfig_file\nNoneType\nNone\nExtension of card file\n\n\n\n\n# Test\nconfig = core.get_config(\"./config/config.env\")\ncard = Card.model_validate(\n    {\n        \"name\": config[\"pingme\"][\"user_input\"][\"card\"][\"name\"],\n        \"context\": config[\"pingme\"][\"user_input\"][\"card\"][\"context\"],\n    }\n)\npingme = PingMe(card, config_file=\"./config/config.env\")\npingme.payload\n\nLets add a test here as well, which will get run through ./.github/workflows/test.yaml whenever changes happen to the repository\n\nsource\n\n\nsend_to_webhook\n\n send_to_webhook (url:str, payload:&lt;module'json'from'/opt/hostedtoolcache/\n                  Python/3.10.16/x64/lib/python3.10/json/__init__.py'&gt;, he\n                  ader:&lt;module'json'from'/opt/hostedtoolcache/Python/3.10.\n                  16/x64/lib/python3.10/json/__init__.py'&gt;={'Content-\n                  Type': 'application/json'})\n\n*Sends a message to a webhook\nArgs: url: str, the webhook URL payload: json, the payload to be sent header: json, the header to be sent\nReturns: json, the response from the webhook*\n\n# Test\nwith fastcore.test.ExceptionExpected():\n    send_to_webhook(\"https://badhost\", json.dumps(pingme.payload))\n\n\nsource\n\n\nPingMe.send_webhook\n\n PingMe.send_webhook ()\n\n\npingme.webhook\n\n\nresponse = requests.post(\n    pingme.webhook[\"url\"],\n    data=json.dumps(pingme.payload),\n    headers={\"Content-Type\": \"application/json\"},\n)\n# empty response is normal\nprint(response.content)\n\n\nsource\n\n\nsend_to_email\n\n send_to_email (payload:&lt;module'json'from'/opt/hostedtoolcache/Python/3.10\n                .16/x64/lib/python3.10/json/__init__.py'&gt;, subject:str,\n                from_:str, to:str, host:str, port:int=25, user=None,\n                password=None)\n\n*Sends a message to an email address\nArgs: payload: json, the payload to be sent subject: str, the subject of the email from_: str, the sender of the email to: str, the recipient of the email host: str, the host of the email server port: int, the port of the email server user: str, the username of the email server password: str, the password of the email server\nReturns: dict, the response from the email server\n\n\n\nNOTE: Wondering if I should do something more like https://learn.microsoft.com/en-us/graph/api/user-sendmail?view=graph-rest-1.0&tabs=http*\n\nsource\n\nPingMe.send_email\n\n PingMe.send_email ()\n\n\nsource\n\n\nsend_to_logfile\n\n send_to_logfile (logfile:str, title:str, text:str)\n\n*Send message to logfile\nArgs: logfile: str, the path to the logfile title: str, the title of the message text: str, the text of the message\nReturns: dict, the response from the logfile\nTODO: The log file only logs title and text right now (not payload), while payload can be included it is not good for parsing. Need to think of a solution for this. Could be to save each payload as a seperate file and the log is a list of files.*\n\nsource\n\n\nPingMe.send_logfile\n\n PingMe.send_logfile ()\n\n\nsource\n\n\ncli\n\n cli (context:str=None, webhook:bool=None, email:bool=None,\n      logfile:bool=None, example:bool=None, config_file:str=None)\n\n*PingMe send a notification to a webhook, email, or log file.\nUsage examples: - basic: pingme –context ‘{“title”:“Test Title”, “text”:“Test Text”}’ –webhook - advanced: pingme –config_file ./config/config.env –context ‘{“title”:“Test Title”, “text”:“Test Text”}’ –webhook –email –logfile –card_name default –card_dir ./cards/ –card_ext .yaml NOTE: Will require use of ./cards/default.yaml and ./config/config.default.env to be set up properly\nReturns: bool, True if successful, False if not*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontext\nstr\nNone\nstring denoting a json object with context variables (e.g. ‘{“title”:“Test Title”, “text”:“Test Text”}’)\n\n\nwebhook\nbool\nNone\nattempts to send to webhook\n\n\nemail\nbool\nNone\nattempts to send to email\n\n\nlogfile\nbool\nNone\nattempts to send to logfile\n\n\nexample\nbool\nNone\nRuns with example params, if it doesn’t work config values haven’t been set properly\n\n\nconfig_file\nstr\nNone\nconfig file to set env vars from\n\n\n\n\ncli(context='{\"title\":\"Test Title\", \"text\":\"Test Text\"}', webhook=True)\n\n\nsource\n\n\ncli_batch\n\n cli_batch (task_file:str=None, config_file:str=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntask_file\nstr\nNone\ntask file to run a batch, used exclusively of other other variables\n\n\nconfig_file\nstr\nNone\nconfig file to set env vars from",
    "crumbs": [
      "Additional Learning Resources"
    ]
  }
]