{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp core\n",
    "# This will create a package named pingme/core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "from nbdev.showdoc import *  # ignore this Pylance warning in favor of following nbdev docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For help with the Markdown language, see [this guide](https://www.markdownguide.org/basic-syntax/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global static vars\n",
    "These are used to modify the template for individual use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# Need the pingme for a few functions, this can be considered a static var\n",
    "\n",
    "import importlib\n",
    "import importlib.util\n",
    "import os\n",
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "PACKAGE_NAME: str = \"pingme\"  # Make sure to adjust this to your package name\n",
    "DEV_MODE: bool = (\n",
    "    False  # set below to override, as this is in an export block it'll be exported while the dev mode section is not\n",
    ")\n",
    "\n",
    "PACKAGE_DIR = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec(PACKAGE_NAME)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    PACKAGE_DIR = os.path.dirname(module.__file__)\n",
    "except ImportError:\n",
    "    DEV_MODE = True\n",
    "except AttributeError:\n",
    "    DEV_MODE = True\n",
    "PROJECT_DIR = os.getcwd()  # override value in dev mode\n",
    "if PROJECT_DIR.endswith(\"nbs\"):\n",
    "    DEV_MODE = True\n",
    "    PROJECT_DIR = os.path.split(PROJECT_DIR)[0]\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"\n",
    "    Base settings class for the package, primarily to gain config_file for dev mode through pydantic\n",
    "    \"\"\"\n",
    "\n",
    "    app_name: str = \"PingMe\"\n",
    "    config_file: str = \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls):\n",
    "        \"\"\"Factory method to create settings based on environment\"\"\"\n",
    "        if DEV_MODE:\n",
    "            return cls(config_file=f\"{PROJECT_DIR}/config/config.env\")\n",
    "        else:\n",
    "            return cls()\n",
    "\n",
    "\n",
    "settings = Settings().create()\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "def setup_logging(log_level=None):\n",
    "    \"\"\"Configure logging for the application\"\"\"\n",
    "    log_level = log_level or (logging.DEBUG if DEV_MODE else logging.INFO)\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "\n",
    "    # Configure root logger\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(log_level)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(console_handler)\n",
    "\n",
    "    # File handler (optional)\n",
    "    if DEV_MODE:\n",
    "        log_dir = os.path.join(PROJECT_DIR, \"logs\")\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        file_handler = logging.FileHandler(os.path.join(log_dir, \"pingme.log\"))\n",
    "        file_handler.setFormatter(formatter)\n",
    "        root_logger.addHandler(file_handler)\n",
    "\n",
    "    # Create a logger for pingme\n",
    "    logger = logging.getLogger(\"pingme\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev mode\n",
    "If you're developing this versus running this, you'll have access to slightly different things. Notable the nbdev functions are only for development and not for runtime. This matters for items such as the config. So we need to detect if you are in dev mode or not and the code has to adjust accordingly. Notice that this section is not exported so will only work in the notebook and not in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section uses nbdev functions so should not be exported as it's for dev purposes\n",
    "import os\n",
    "\n",
    "if DEV_MODE:\n",
    "    PACKAGE_DIR = nbdev.config.get_config(cfg_name=\"settings.ini\", path=os.getcwd())[\n",
    "        \"lib_path\"\n",
    "    ]  # the library is the package of course\n",
    "    PROJECT_DIR = nbdev.config.get_config(\n",
    "        cfg_name=\"settings.ini\", path=os.getcwd()\n",
    "    ).config_path  # the default location of nbdev config file (settings.ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    " A module which contains common functions to be used by other modules. Those that exist in the template are meant to be common functions we can use against multiple packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "Notebook blocks starting with #|hide are not shown in the documentation and not exported to the python package. Blocks with #|export are exported to the python package. Blocks with neither are shown to the documentation but not exported to the python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Currently all libraries included are listed at the top and calls to them are also made in the block of code that uses them. This is for readability and the performance hit of the import is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# standard libs\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Common to template\n",
    "# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`\n",
    "import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/\n",
    "import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml\n",
    "from fastcore import (\n",
    "    test,\n",
    ")\n",
    "from fastcore.script import (\n",
    "    call_parse,\n",
    ")  # for @call_parse, https://fastcore.fast.ai/script\n",
    "\n",
    "# Project specific libraries\n",
    "import shutil  # using shell utilities\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, field_validator\n",
    "from pathlib import Path\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Our config file holds all program and user specific variables. This is a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production. This is also a good practice to follow as it allows us to easily change variables without having to change code. It also allows us to easily change variables based on the environment we are running in. For example, we may want to run a program in a test environment with a different database than we would in production.\n",
    "\n",
    "Configuration is templated to rely on environment (ENV) variables. A default ENV config is provided in `./config/config.default.env` and more advanced data structures are supported in `./config/config.default.yaml`. The `.yaml` file is meant to represent what your program actually works with and the `.env` file options the user can change at run time.\n",
    "\n",
    "Make sure you know the priority of variables and check on them when debugging your code. Also ensure that your yaml file is referenced appropriately in the `.env` file. \n",
    "\n",
    "When in use there's an expectation you'll have multiple config files for different use cases e.g. development, production environment for different paths, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set env variables\n",
    "A helper function for getting your config values, this will set the environment variables with the provided `.env` values. If you're missing values it'll ensure they're loaded in with the defaults file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def set_env_variables(config_path: str, overide_env_vars: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Load dot env sets environmental values from a file, if the value already exists it will not be overwritten unless override is set to True.\n",
    "    If we have multiple .env files then we need to apply the one which we want to take precedence last with overide.\n",
    "\n",
    "    Order of precedence: .env file > environment variables > default values\n",
    "    When developing, making a change to the config will not be reflected until the environment is restarted\n",
    "\n",
    "    Set the env vars first, this is needed for the card.yaml to replace ENV variables\n",
    "    NOTE: You need to adjust PROJECT_NAME to your package name for this to work, the exception is only for dev purposes\n",
    "    This here checks if your package is installed, such as through pypi or through pip install -e  [.dev] for development. If it is then it'll go there and use the config files there as your default values.\n",
    "\n",
    "    Args:\n",
    "    config_path (str): path to the config file\n",
    "    overide_env_vars (bool): if True, will overwrite existing env variables\n",
    "\n",
    "    Returns:\n",
    "    bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dotenv.load_dotenv(f\"{PACKAGE_DIR}/config/config.default.env\", override=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {PACKAGE_DIR}/config/config.default.env does not exist\")\n",
    "        return False\n",
    "\n",
    "    # 2. set values from file:\n",
    "    if os.path.isfile(config_path):\n",
    "        dotenv.load_dotenv(config_path, override=overide_env_vars)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get config\n",
    "\n",
    "When you run this function, assuming things are set up properly, you end up with a dict that matches your `.yaml` file. This file will have all the inputs for the package and settings of your program.\n",
    "\n",
    "To do this it will use a `.env` config file, which has an associated yaml file defined with `CORE_YAML_CONFIG_FILE` in the `.env` file. And then use the `.env` file to load values into the associated `.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_config(config_path: str = None, overide_env_vars: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Load the config.env from the config path, the config.env should reference the config.yaml file, which will be loaded and returned as\n",
    "    a dictionary. The config.yaml file should be in the same directory as the config.env file.\n",
    "\n",
    "    Args:\n",
    "    config_path (str): The path to the config.env file\n",
    "    overide_env_vars (bool): If the env vars should be overriden by the config.yaml file\n",
    "\n",
    "    Returns:\n",
    "    dict: The config.yaml file as a dictionary, it'll also replace any ENV variables in the yaml file\n",
    "    \"\"\"\n",
    "    if config_path is None:\n",
    "        config_path = \"\"\n",
    "    # First sets environment with variables from config_path, then uses those variables to fill in appropriate values in the config.yaml file, the yaml file is then returned as a dict\n",
    "    # If you want user env variables to take precedence over the config.yaml file then set overide_env_vars to False\n",
    "    set_env_variables(config_path, overide_env_vars)\n",
    "\n",
    "    config: dict = envyaml.EnvYAML(\n",
    "        os.environ.get(\n",
    "            \"CORE_YAML_CONFIG_FILE\", f\"{PACKAGE_DIR}/config/config.default.yaml\"\n",
    "        ),\n",
    "        strict=False,\n",
    "    ).export()\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "All the user input variables and machine adjustable variables should be in your config, which is a dict. Reference config.default.yaml for how to access your variables. Also note that with python dicts you can use `dict_variable.get(\"variable\", default_value)` to ensure that you don't get a key error if the variable is not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# create a os.PathLike object\n",
    "config = get_config(os.environ.get(\"CORE_CONFIG_FILE\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show project env vars\n",
    "A helper function intended to only be used with debugging. It shows all your project specific environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def show_project_env_vars(config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Show all the project environment variables, this is useful for debugging and seeing what is being set\n",
    "\n",
    "    Args:\n",
    "    config (dict): The dictionary of all the environment variables\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for k, v in config.items():\n",
    "        # If ENV var starts with PROJECTNAME_ then print\n",
    "        if k.startswith(config[\"CORE_PROJECT_VARIABLE_PREFIX\"]):\n",
    "            print(f\"{k}={v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# checking local vars\n",
    "show_project_env_vars(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# import shutil # called at top\n",
    "def tool_is_present(tool_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a tool is present in the current environment\n",
    "\n",
    "    Args:\n",
    "    tool_name (str): The name of the tool to check\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the tool is present, False otherwise\n",
    "    \"\"\"\n",
    "    return shutil.which(tool_name) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# import sys # for stderr, called at top\n",
    "def tools_are_present(tool_names: list) -> bool:\n",
    "    \"\"\"\n",
    "    Check if all tools are present in the current environment\n",
    "\n",
    "    Args:\n",
    "    tool_names (list): A list of tools to check\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all tools are present, False otherwise\n",
    "    \"\"\"\n",
    "    tools_present: bool = True\n",
    "    for tool in tool_names:\n",
    "        if not tool_is_present(tool):\n",
    "            print(f\"Tool {tool} is not present in current environment\", file=sys.stderr)\n",
    "            tools_present = False\n",
    "    return tools_present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_samplesheet\n",
    "This function is to unify the way we work with sample_sheet's which is for us a file with a table of values, typically samples for batch processing. We want to approach doing it this way so all programs have batch processing in mind and working with the same data structure.\n",
    "\n",
    "To make use of it we have a small sample_sheet yaml object which looks like\n",
    "    \n",
    "```yaml\n",
    "sample_sheet:\n",
    "    path: path/to/sample_sheet.tsv\n",
    "    delimiter: '\\t' # Optional, will assume , for csv and \\t otherwises\n",
    "    header: 0 # Optional, 0 indicates first row is header, None indicates no header\n",
    "    columns: ['column1', 'column2', 'column3'] # Optional, if not provided all columns will be used\n",
    "```\n",
    "\n",
    "Make sure to add that to your relevant section in your config (can be multiple times if you're working with different sheets or different columns), then call the function on this object and it'll either mention somethings wrong or return a pandas dataframe with the columns of interest.\n",
    "\n",
    "This is an example of a common sample_sheet we work with. We will ingest the hash at the beginning so it doesn't affect column naming. Extra empty rows at the end are also stripped.\n",
    "```tsv\n",
    "#sample_id\tfile_path\tmetadata1\tmetadata2\n",
    "Sample1\t/path/to/sample1.fasta\tvalue1\toption1\n",
    "Sample2\t/path/to/sample2.fasta\tvalue2\toption2\n",
    "Sample3\t/path/to/sample3.fasta\tvalue3\toption1\n",
    "Sample4\t/path/to/sample4.fasta\tvalue1\toption2\n",
    "Sample5\t/path/to/sample5.fasta\tvalue2\toption1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# library imports handled at top\n",
    "# import pandas as pd\n",
    "# from pydantic import BaseModel, field_validator\n",
    "# from pathlib import Path\n",
    "# from typing import Any\n",
    "class SamplesheetConfig(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration class for loading a sample sheet into a pandas dataframe\n",
    "\n",
    "    Extends:\n",
    "    BaseModel\n",
    "    \"\"\"\n",
    "\n",
    "    path: str\n",
    "    delimiter: str = \"\\t\"\n",
    "    header: int = 0\n",
    "    columns: list | None = None\n",
    "\n",
    "    # Custom validator to check if the file exists\n",
    "    @field_validator(\"path\")\n",
    "    def check_file_exists(cls, value):\n",
    "        if not Path(value).is_file():\n",
    "            raise ValueError(f\"The file at path '{value}' does not exist.\")\n",
    "        return value\n",
    "\n",
    "    # Override __init__ to accept a dictionary directly, for backwards compatibility probably should just use parse_obj\n",
    "    def __init__(self, config: dict[str, Any]):\n",
    "        super().__init__(**config)  # Unpack dictionary internally\n",
    "\n",
    "\n",
    "def get_samplesheet(samplesheet_config: SamplesheetConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the sample sheet into a pandas dataframe\n",
    "    If columns is not None then it will only load those columns\n",
    "    If the sample sheet is a csv then it will load it as a csv, otherwise it will assume it's a tsv\n",
    "\n",
    "    Expected samplesheet_config:\n",
    "    sample_sheet:\n",
    "      path: path/to/sample_sheet.tsv\n",
    "      delimiter: '\\t' # Optional, will assume , for csv and \\t otherwises\n",
    "      header: 0 # Optional, 0 indicates first row is header, None indicates no header\n",
    "      columns: ['column1', 'column2', 'column3'] # Optional, if not provided all columns will be used\n",
    "\n",
    "    Example sample sheet:\n",
    "    #sample_id\tfile_path\tmetadata1\tmetadata2\n",
    "    Sample1\t/path/to/sample1.fasta\tvalue1\toption1\n",
    "    Sample2\t/path/to/sample2.fasta\tvalue2\toption2\n",
    "    Sample3\t/path/to/sample3.fasta\tvalue3\toption1\n",
    "    Sample4\t/path/to/sample4.fasta\tvalue1\toption2\n",
    "    Sample5\t/path/to/sample5.fasta\tvalue2\toption1\n",
    "\n",
    "    Args:\n",
    "    samplesheet_config (SamplesheetConfig): The configuration for loading the sample sheet\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The sample sheet as a pandas dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # note when we have a header the first column may begin with a #, so we need to remove it\n",
    "        df = pd.read_csv(\n",
    "            samplesheet_config.path,\n",
    "            delimiter=samplesheet_config.delimiter,\n",
    "            header=samplesheet_config.header,\n",
    "            comment=None,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Error: Could not load sample sheet into dataframe, you have a problem with your sample sheet or the configuration.\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    # Check the first header has a # in it, if so remove it for only that item\n",
    "    if df.columns[0].startswith(\"#\"):\n",
    "        df.columns = [col.lstrip(\"#\") for col in df.columns]\n",
    "    # Ensure the sample sheet has the correct columns\n",
    "    if samplesheet_config.columns is not None and not all(\n",
    "        [col in df.columns for col in samplesheet_config.columns]\n",
    "    ):\n",
    "        raise ValueError(\"Error: Sample sheet does not have the correct columns\")\n",
    "    # also drop columns which are not needed\n",
    "    if samplesheet_config.columns is not None:\n",
    "        df = df[samplesheet_config.columns]\n",
    "\n",
    "    # Clean the df of any extra rows that can be caused by empty lines in the sample sheet\n",
    "    df = df.dropna(how=\"all\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"path\": f\"{PROJECT_DIR}/input/example_samplesheet.tsv\",\n",
    "    \"delimiter\": \"\\t\",\n",
    "    \"columns\": None,\n",
    "}\n",
    "\n",
    "samplesheet_config = SamplesheetConfig(config_dict)\n",
    "\n",
    "print(get_samplesheet(samplesheet_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are **not** tempalted and you should adjust this with your own code. It's included as an example of how to code some functions with associated tests and how to make it work on the command line. It is best to code by creating a new workbook and then importing the functions of this into that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def hello_world(name: str = \"Not given\") -> str:\n",
    "    \"\"\"\n",
    "    A simple function that returns a hello world message with a name, for testing purposes\n",
    "    \"\"\"\n",
    "    return f\"Hello World! My name is {name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This here is a a test as part of fastcore.test, all fastcore tests will be automatically run when doing nbdev_test as well as through github actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_eq(\"Hello World! My name is Kim\", hello_world(\"Kim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @call_parse will, with the settings.ini entry way, automatically transform your function into a command line tool. Comments of the functions will appear for help messages and the initial docstring will appear in the help as well. You can also define defaults for the arguments and should define a typehint to control inputs. The function will likely have to resolve variables with ENV vars and config files. The recommended way to do this is to assume variables passed here are a higher priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "# from fastcore.script import call_parse # called at top, with settings.ini it will let you call it from the command line\n",
    "@call_parse\n",
    "def cli(\n",
    "    name: str,  # Your name\n",
    "    config_file: str = None,  # config file to set env vars from\n",
    "):\n",
    "    \"\"\"\n",
    "    This will print Hello World! with your name\n",
    "\n",
    "    Args:\n",
    "    name (str): Your name\n",
    "    config_file (str): The path to the config file, if not provided it will use the default config file\n",
    "    \"\"\"\n",
    "    config = get_config(config_file)  # Set env vars and get config variables\n",
    "    if name is not None:\n",
    "        config[\"example\"][\"input\"][\"name\"] = name\n",
    "\n",
    "    print(hello_world(config[\"example\"][\"input\"][\"name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with potentially variable input to confirm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_eq(\n",
    "    \"Hello World! My name is Kim\", hello_world(config[\"example\"][\"input\"][\"name\"])\n",
    ")\n",
    "test.test_eq(None, cli(\"Kim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def create_bash_script(script_path, script_content, slurm_params=None):\n",
    "    \"\"\"\n",
    "    Create a bash script with the given content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add slurm headers to the script\n",
    "    if slurm_params:\n",
    "        with open(script_path, \"r\") as script_file:\n",
    "            script_content = script_file.read()\n",
    "        script_content = (\n",
    "            \"#!/bin/bash\\n\"\n",
    "            + \"\\n\".join(\n",
    "                [f\"#SBATCH --{key}={value}\" for key, value in slurm_params.items()]\n",
    "            )\n",
    "            + \"\\n\"\n",
    "            + script_content\n",
    "        )\n",
    "        with open(script_path, \"w\") as script_file:\n",
    "            script_file.write(script_content)\n",
    "    else:\n",
    "        with open(script_path, \"w\") as script_file:\n",
    "            script_file.write(script_content)\n",
    "\n",
    "\n",
    "def execute_job(script_path, use_slurm=False, slurm_params=None):\n",
    "    \"\"\"\n",
    "    Executes a bash script either locally (bash) or via Slurm.\n",
    "    \"\"\"\n",
    "    if use_slurm:\n",
    "        # Build sbatch command\n",
    "        sbatch_command = [\"sbatch\"]\n",
    "        if slurm_params:\n",
    "            for key, value in slurm_params.items():\n",
    "                sbatch_command.append(f\"--{key}={value}\")\n",
    "        sbatch_command.append(script_path)\n",
    "\n",
    "        # Submit to Slurm\n",
    "        result = subprocess.run(sbatch_command, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Job submitted to Slurm: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"Error submitting to Slurm: {result.stderr.strip()}\")\n",
    "    else:\n",
    "        # Run locally\n",
    "        result = subprocess.run([\"bash\", script_path], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Script executed locally: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"Error executing script locally: {result.stderr.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(script_path, dependency=None) -> str | None:\n",
    "    \"\"\"\n",
    "    Submit a Slurm job with optional dependencies.\n",
    "\n",
    "    Args:\n",
    "    script_path (str): Path to the script to submit.\n",
    "    dependency (str): Job ID to depend on.\n",
    "\n",
    "    Returns:\n",
    "    str: Job ID if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    command = [\"sbatch\"]\n",
    "    if dependency:\n",
    "        command.append(f\"--dependency=afterok:{dependency}\")\n",
    "    command.append(script_path)\n",
    "\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        job_id = result.stdout.strip().split()[-1]  # Extract job ID from output\n",
    "        print(f\"Job submitted: {job_id}\")\n",
    "        return job_id\n",
    "    else:\n",
    "        print(f\"Error submitting job: {result.stderr.strip()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_workflow(scripts) -> str | None:\n",
    "    \"\"\"\n",
    "    Submit a series of scripts with dependencies.\n",
    "\n",
    "    Args:\n",
    "    scripts: List of script paths to submit, they will be submitted in order with previous job as a dependency.\n",
    "\n",
    "    Returns:\n",
    "    str: Job ID of the last submitted job if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    previous_job_id = None\n",
    "    for script in scripts:\n",
    "        dependency = (  # Construct the dependency argument if there is a previous job\n",
    "            f\"--dependency=afterok:{previous_job_id}\" if previous_job_id else None\n",
    "        )\n",
    "        previous_job_id = submit_job(script, dependency)\n",
    "    return previous_job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# This is included at the end to ensure when you run through your notebook the code is also transferred to the associated python package\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
